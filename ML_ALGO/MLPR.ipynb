{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feather\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset Working Directory\n",
    "# os.chdir(\"c:\\\\Users\\\\Andrew\\\\Documents\\\\Uni Trier\\\\Semester 3\\\\Case Study\\\\ML Algo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set top level directory, path for reading feather and output file for the results\n",
    "top_wd = os.getcwd()\n",
    "feather_dir = top_wd + \"\\\\Feather\"\n",
    "os.makedirs(top_wd + \"\\\\Results\", exist_ok=True)\n",
    "results_dir = top_wd + \"\\\\Results\"\n",
    "holdout_dir = str(Path(top_wd).parents[0]) + \"\\\\AMELIA\\\\AMELIA_P_level_v0.2.3 (Person-Level)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for numeric conversion of sex variable\n",
    "def to_numeric(dataframe):\n",
    "    sex = {\"Male\":0,\"Female\":1}\n",
    "    dataframe = dataframe.replace({\"Sex\": sex})\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to hotcode all catagorical variables\n",
    "def cata_encode(dataframe):\n",
    "    \"\"\"Since get_dummies has not dropped the extra column, it must be done explicitly (Avoids Multicolinearity)\"\"\"\n",
    "    #Cities will be dropped since there are too many and they are not considered relevant for our analysis\n",
    "    #4: Other inactive person \n",
    "    Work_Columns = {\"Work_Status_1.0\":\"At Work\", \"Work_Status_2.0\":\"Unemployed\", \"Work_Status_3.0\":\"Retired\"}\n",
    "    #5: ISCED5 or ISCED6\n",
    "    Highest_ISCED = {\"Highest_ISCED_1.0\":\"ISCED 1\", \"Highest_ISCED_2.0\":\"ISCED 2\", \"Highest_ISCED_3.0\":\"ISCED 3\", \"Highest_ISCED_4.0\":\"ISCED 4\"}\n",
    "    #5: Divorced\n",
    "    Martial_Status = {\"Martial_Status_1.0\":\"Never Married\", \"Martial_Status_2.0\":\"Married\",\"Martial_Status_3.0\":\"Separated\",\"Martial_Status_4.0\":\"Widowed\"}\n",
    "    #4: Region 4\n",
    "    Region_ID = {\"Regional_ID_1\":\"Region_1\", \"Regional_ID_2\":\"Region_2\", \"Regional_ID_3\":\"Region_3\"}\n",
    "    #11: Province 11\n",
    "    #40: District 40\n",
    "\n",
    "    dataframe = pd.get_dummies(dataframe, columns=['Work_Status', \"Highest_ISCED\", \"Martial_Status\", \"Regional_ID\", \"Province\", \"District\"]).rename(columns=Work_Columns).rename(columns=Highest_ISCED).rename(columns=Martial_Status).rename(columns=Region_ID).drop(columns=[\"Work_Status_4.0\", \"Highest_ISCED_5.0\", \"Martial_Status_5.0\", \"City.Community\", \"Regional_ID_4\", \"Province_11\", \"District_40\"])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store Holdout set for testing\n",
    "os.chdir(holdout_dir)\n",
    "Holdout = feather.read_dataframe(\"Holdout.feather\") \n",
    "Holdout_y = Holdout[\"Person_Income\"]\n",
    "Holdout_x = Holdout.drop(columns = [\"index\", \"Person_Income\", \"Personal_ID\"])\n",
    "\n",
    "sc = StandardScaler()\n",
    "Holdout_unscaled = Holdout_x\n",
    "Holdout_x = to_numeric(Holdout_x)\n",
    "Holdout_x = cata_encode(Holdout_x)\n",
    "Holdout_x = sc.fit_transform(Holdout_x)\n",
    "Holdout_x = pd.DataFrame(Holdout_x, columns=Holdout_unscaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_Methods = [\"SRS\", \"Stratified\", \"Cluster\"]\n",
    "#Create a dictionary for accesing all dataframes\n",
    "df_dict = {}\n",
    "for j in S_Methods:\n",
    "    for i in range(10):\n",
    "        os.chdir(feather_dir)\n",
    "        SRS = feather.read_dataframe(f\"0{i+1}_{j}_base_sample.feather\") \n",
    "        Importance = feather.read_dataframe(f\"0{i+1}_{j}_importance_sample.feather\") \n",
    "        Synthetic = feather.read_dataframe(f\"0{i+1}_{j}_synthetic_sample.feather\")\n",
    "\n",
    "        #Apply numeric conversion (Male:1, Female:2)\n",
    "        SRS = to_numeric(SRS)\n",
    "        SRS = cata_encode(SRS)\n",
    "        Importance = to_numeric(Importance)\n",
    "        Importance = cata_encode(Importance)\n",
    "        Synthetic = to_numeric(Synthetic)\n",
    "        Synthetic = cata_encode(Importance)\n",
    "\n",
    "        df_dict[f\"{j}_Base_{i+1}\"] = SRS\n",
    "        df_dict[f\"{j}_Importance_{i+1}\"] = Importance\n",
    "        df_dict[f\"{j}_Synthetic_{i+1}\"] = Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function for handeling train test split\n",
    "def t_t_split(dataframe):\n",
    "    #Split x and y vars (also drop personal ID identifier and index)\n",
    "    x_var = dataframe.iloc[:,1:].drop(columns = [\"Personal_ID\", \"index\"])\n",
    "    y_var = dataframe.iloc[:,0]\n",
    "\n",
    "    #Creat Train/Test split for x and y\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_var, y_var, test_size = 0.25, random_state = 420)\n",
    "\n",
    "    #Keep a copy of unscaled x_test for later comparison\n",
    "    x_test_unscaled = x_test\n",
    "\n",
    "    #Scale x var for train and test\n",
    "    sc = StandardScaler()\n",
    "    #Done now for convience regarding column reassignment(rather than above when x_var created)\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.fit_transform(x_test)\n",
    "    x_train = pd.DataFrame(x_train, columns=x_var.columns)\n",
    "    x_test = pd.DataFrame(x_test, columns=x_var.columns)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, x_test_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through Sampling methods and run our NN on each one, storing the score and results\n",
    "def results(df_dict):\n",
    "   S_Methods = [\"SRS\", \"Stratified\", \"Cluster\"]\n",
    "   s_names = [\"Base\", \"Importance\", \"Synthetic\"]\n",
    "\n",
    "   scores = pd.DataFrame(columns = s_names)\n",
    "   scores_holdout = pd.DataFrame(columns = s_names)\n",
    "   predicts_dict = {}\n",
    "   holdout_p_dict = {}\n",
    "\n",
    "   for S_Method in S_Methods:\n",
    "      for Method in s_names:\n",
    "         for i in range(10):   \n",
    "            x_train, x_test, y_train, y_test, x_test_unscaled = t_t_split(df_dict[f\"{S_Method}_{Method}_{i+1}\"])\n",
    "\n",
    "            #Run Multi-layer Perceptron regressor\n",
    "            regr = MLPRegressor(random_state=420, max_iter=500, activation = \"relu\", solver='lbfgs').fit(x_train, y_train)\n",
    "            \n",
    "            \"\"\"Write a scoring and predictions function for holdout set\"\"\"\n",
    "            #Generate Score Value\n",
    "            s_v = regr.score(x_test, y_test)\n",
    "            h_s_v = regr.score(Holdout_x, Holdout_y)\n",
    "\n",
    "            #Add score to scores DataFrame\n",
    "            if S_Method == \"SRS\"\n",
    "               if Method == \"Base\":\n",
    "                  scores = scores.append({f\"{S_Method}_{Method}\":s_v}, ignore_index=True)\n",
    "                  scores_holdout = scores_holdout.append({f\"{S_Method}_{Method}\":h_s_v}, ignore_index=True)\n",
    "\n",
    "            #Once SRS Base is done, scores are updated with this function (To avoid indexing errors) \n",
    "            if S_Method != \"SRS\" | Method != \"Base\":\n",
    "               scores.loc[i,f\"{S_Method}_{Method}\"] = s_v\n",
    "               scores_holdout.loc[i,f\"{S_Method}_{Method}\"] = h_s_v\n",
    "\n",
    "            #Generate Predictions\n",
    "            predictions = regr.predict(x_test)\n",
    "            holdout_p = regr.predict(Holdout_x)\n",
    "            \n",
    "            #Replace negative outputs with 0\n",
    "            predictions = np.where(predictions < 0, 0, predictions)\n",
    "            holdout_p = np.where(holdout_p < 0, 0, holdout_p)\n",
    "\n",
    "            #Convert Predicitons from Ndarray to Dataframe for Concat\n",
    "            predictions = pd.DataFrame(data=predictions, columns=[\"Predictions\"])\n",
    "            holdout_p = pd.DataFrame(data=holdout_p, columns=[\"Predictions\"])\n",
    "            \n",
    "            #Store Y_test, Predictions for X_test and data for X_test (For Comparision)\n",
    "            fused_df = pd.concat([y_test.reset_index(drop=True), predictions, x_test_unscaled.reset_index(drop=True)], axis = 1)\n",
    "            fused_h_df = pd.concat([Holdout_y.reset_index(drop=True), holdout_p, Holdout_unscaled.reset_index(drop=True)], axis = 1)\n",
    "            predicts_dict[f\"{S_Method}_{Method}_{i+1}\"] = fused_df\n",
    "            holdout_p_dict[f\"{S_Method}_{Method}_{i+1}\"] = fused_h_df\n",
    "                  \n",
    "   return scores, predicts_dict, scores_holdout, holdout_p_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runtime for SRS: 49 minutes, 25 seconds\n",
    "scores, predicts_dict, scores_holdout, holdout_p_dict = results(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has already been referenced previously but here just in case its missed\n",
    "S_Methods = [\"SRS\", \"Stratified\", \"Cluster\"]\n",
    "os.chdir(results_dir)\n",
    "scores.index += 1\n",
    "scores_holdout.index += 1\n",
    "feather.write_dataframe(scores,\"scores.feather\")\n",
    "feather.write_dataframe(scores_holdout,\"scores_holdout.feather\")\n",
    "for j in S_Methods:\n",
    "    for i in range(10):\n",
    "        feather.write_dataframe(predicts_dict[f\"{j}_Base_{i+1}\"],f\"0{i+1}_SRS_MLPR_results.feather\") \n",
    "        feather.write_dataframe(holdout_p_dict[f\"{j}_Base_{i+1}\"],f\"0{i+1}_SRS_Holdout_results.feather\") \n",
    "        feather.write_dataframe(predicts_dict[f\"{j}_Importance_{i+1}\"],f\"0{i+1}_Importance_MLPR_results.feather\") \n",
    "        feather.write_dataframe(holdout_p_dict[f\"{j}_Importance_{i+1}\"],f\"0{i+1}_Importance_Holdout_results.feather\") \n",
    "        feather.write_dataframe(predicts_dict[f\"{j}_Synthetic_{i+1}\"],f\"0{i+1}_Synthetic_MLPR_results.feather\")\n",
    "        feather.write_dataframe(holdout_p_dict[f\"{j}_Synthetic_{i+1}\"],f\"0{i+1}_Synthetic_Holdout_results.feather\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c14cf6fee9e82c9cdd34ec15f18624274457bb0e780094f6e02ebebdc5a43f41"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('amelia': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
